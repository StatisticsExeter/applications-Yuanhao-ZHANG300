---
title: "Supervised Learning Coursework Report"
author: "Yuanhao Zhang"
format:
  html:
    toc: true
    code-fold: true
execute:
  working-directory: ../../
---

```{python}
import matplotlib
matplotlib.use("module://matplotlib_inline.backend_inline")  # no need to print anything
```


## 1 Introduction

This report uses the dataset `data_cache/energy.csv` to build a binary classification model that predicts whether a property is **“Pre-30s”** or **“Post-30s”**.  
Given energy efficiency, consumption, emissions and cost indicators, the goal is to estimate the approximate age category of a house and compare the performance of several supervised learning models.

```{python}
from pathlib import Path
import pandas as pd
import os

data_path = Path("../../data_cache/energy.csv")
print("Reading from:", data_path.resolve())

energy = pd.read_csv(data_path)
energy.head()
```


## 2 Exploratory Data Analysis (EDA)

First, we examine summary statistics and visualise the distributions of key numerical features.
```{python}
energy.describe(include="all")
```

```{python}
import matplotlib.pyplot as plt

numeric_cols = [
    "current_energy_efficiency",
    "environment_impact_current",
    "energy_consumption_current",
    "co2_emissions_current",
]

fig, axes = plt.subplots(1, len(numeric_cols), figsize=(14, 3))
for ax, col in zip(axes, numeric_cols):
    ax.hist(energy[col], bins=20)
    ax.set_title(col)

fig.tight_layout()
plt.show()
```

Overall, the variables show roughly unimodal distributions with no extreme outliers,
suggesting that standardised numerical modelling is appropriate.

We also plot a scatter diagram to inspect relationships between energy consumption and emissions.

```{python}
import seaborn as sns

sns.relplot(
data=energy,
x="energy_consumption_current",
y="co2_emissions_current",
hue="built_age"
)

plt.show()

```

The two classes overlap considerably, meaning a machine learning model is needed
to combine multiple indicators and improve discrimination.


## 3 Baseline Model: Logistic Regression

We treat built_age as the target, and numerical energy metrics as features.

```{python}
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, ConfusionMatrixDisplay

X = energy[numeric_cols]
y = energy["built_age"]

X_train, X_test, y_train, y_test = train_test_split(
X, y, test_size=0.3, random_state=42, stratify=y
)

logit_clf = make_pipeline(
StandardScaler(),
LogisticRegression(max_iter=1000)
)
logit_clf.fit(X_train, y_train)
y_pred_logit = logit_clf.predict(X_test)

print(classification_report(y_test, y_pred_logit))
ConfusionMatrixDisplay.from_estimator(logit_clf, X_test, y_test)

plt.show()

```

Logistic regression provides a stable linear baseline model, but misclassification still exists due to overlapping class distributions.

The logistic regression baseline achieves an accuracy of **0.74**, but this result is misleading.  
Because the dataset is **class-imbalanced**, the model predicts **all samples as “Post-30s”**, the majority class.  

This indicates that logistic regression fails to separate the two classes and learns a degenerate strategy:  
**always predict the majority class**.

## 4 Alternative Models: Decision Tree and Random Forest

To capture potential nonlinear relationships, we test two tree-based models.

```{python}
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

dt_clf = DecisionTreeClassifier(max_depth=4, random_state=42)
dt_clf.fit(X_train, y_train)
y_pred_dt = dt_clf.predict(X_test)

rf_clf = RandomForestClassifier(
n_estimators=200,
random_state=42
)
rf_clf.fit(X_train, y_train)
y_pred_rf = rf_clf.predict(X_test)

print("Decision Tree:")
print(classification_report(y_test, y_pred_dt))

print("Random Forest:")
print(classification_report(y_test, y_pred_rf))
```

As expected, the random forest tends to outperform both logistic regression
and a single decision tree, providing better recall and F1 scores.


## 5 Enhanced Visualisation: Feature Importance

We inspect which features contribute most to the random forest’s predictions.

```{python}
import numpy as np

importances = rf_clf.feature_importances_
order = np.argsort(importances)[::-1]

plt.figure(figsize=(6, 4))
plt.bar(range(len(numeric_cols)), importances[order])
plt.xticks(range(len(numeric_cols)), [numeric_cols[i] for i in order], rotation=45)
plt.ylabel("Feature importance")
plt.tight_layout()
plt.show()
```

Energy consumption, efficiency and CO₂ emissions rank highest, which aligns with domain intuition: older houses tend to have poorer efficiency.

## 6 Conclusion

This supervised learning task shows:

- Logistic regression provides a useful linear baseline.
- Decision trees reveal nonlinear relationships but may overfit.
- Random forests deliver the strongest overall performance.

Energy efficiency and emissions indicators are effective predictors of housing age,
but applying such models operationally would require richer data (e.g., geographic or structural attributes) and more robust validation.

### **Future Work**

To address the class imbalance and improve minority-class recognition, several extensions could be explored:

- using `class_weight="balanced"` in logistic regression,  
- adjusting the decision threshold instead of fixing it at 0.5,  
- applying resampling techniques such as oversampling or SMOTE,  
- collecting more representative data across housing ages.

These techniques could help capture the *Pre-30s* class more effectively and improve robustness.