---
title: "Unsupervised Learning Coursework Report"
author: "Yuanhao Zhang"
format:
  html:
    toc: true
    code-fold: true
execute:
  working-directory: ../../
---

## 1 Introduction

This report applies unsupervised learning techniques to the la_collision.csv datase. This dataset contains a summary of traffic accident statistics for the 30 Local Authorities (LADs) in the UK. These aggregated attributes reflect different environmental conditions and road characteristics, allowing us to explore whether LADs form natural groups based on their collision profiles.
Since the data does not contain labels, methods such as K-Means and hierarchical clustering can help discover the underlying structure.

We first load the data.

```{python}
import os
from pathlib import Path
import pandas as pd

proj_root = Path(os.getenv("QUARTO_PROJECT_DIR", ".")).resolve()
print("QUARTO_PROJECT_DIR =", os.getenv("QUARTO_PROJECT_DIR"))
print("Resolved project root =", proj_root)

data_path = Path("../../data_cache/la_collision.csv")
print("Reading from:", data_path.resolve())

la_energy = pd.read_csv(data_path)
la_energy.head()
```

## 2 Exploratory Data Analysis (EDA)

We inspect basic summary statistics and the distribution of key numerical features.

```{python}
la_energy.describe(include="all")
```

Statistical results show significant differences between different administrative regions. These large fluctuations indicate that LADs vary considerably in terms of weather conditions, road structure, and traffic density.

```{python}
numeric_cols = [col for col in la_energy.columns if la_energy[col].dtype != "object"]
numeric_cols[:8]

import matplotlib.pyplot as plt

fig, axes = plt.subplots(2, 2, figsize=(8, 6))
axes = axes.ravel()

for ax, col in zip(axes, numeric_cols[:4]):
    ax.hist(la_energy[col].dropna(), bins=20)
    ax.set_title(col)

for ax in axes[len(numeric_cols[:4]):]:
    ax.set_visible(False)

fig.suptitle("Distributions of Selected Collision Variables", y=1.02)
fig.tight_layout()
plt.show()
```

The histogram shows that most variables exhibit a right-skewed distribution: the number of accidents in most LADs is at a low to medium level, while the number of accidents in a few areas is abnormally high. This heterogeneity suggests that LADs are likely to naturally cluster based on accident characteristics, thus making clustering a suitable method.

## 3 Baseline Clustering: K-Means

We standardise the numerical features and apply K-Means clustering.

```{python}
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

X = la_energy[numeric_cols].dropna()
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

kmeans = KMeans(n_clusters=3, random_state=42)
labels = kmeans.fit_predict(X_scaled)

X_km = X.copy()
X_km["cluster"] = labels
X_km["cluster"].value_counts()
```

After standardization, K-Means (k=3) clearly clustered the LADs into three clusters.

### Visualising Clusters in 2D (PCA)

```{python}
from sklearn.decomposition import PCA
import seaborn as sns

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(6, 5))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1],
hue=labels, palette="viridis", s=20)
plt.title("K-Means Clusters (PCA Projection)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend(title="Cluster")
plt.show()
```

PCA visualization shows clear separation between clusters, indicating that the first two principal components successfully captured the main differences between LADs.

Cluster 2 is tightly clustered in the graph, representing a small class of high-risk areas; while clusters 0 and 1 are more dispersed, suggesting a more complex internal structure.  

## 4 Alternative Model: Hierarchical Clustering

To compare with K-Means, we apply hierarchical clustering on a smaller sample.

```{python}
from scipy.cluster.hierarchy import dendrogram, linkage

n_samples = min(300, len(X))
sample_X = X.sample(n=n_samples, random_state=42)

sample_scaled = scaler.transform(sample_X)

Z = linkage(sample_scaled, method="ward")

plt.figure(figsize=(10, 5))
dendrogram(Z, no_labels=True, color_threshold=0.7 * max(Z[:, 2]))
plt.title("Hierarchical Clustering Dendrogram (Sample)")
plt.xlabel("Sample index")
plt.ylabel("Distance")
plt.show()
```

The dendrogram suggests two or three major branches splitting early, aligning closely with the K-Means result, which indicates robust cluster validity with the agreement between two different clustering frameworks.

## 5 Enhanced Visualisation and Interpretation

We briefly examine how clusters relate to a relevant categorical feature, such as collision severity (if available).

```{python}
if "severity" in la_energy.columns:
    coll_clusters = la_energy.loc[X.index].copy()
    coll_clusters["cluster"] = labels

    plt.figure(figsize=(6, 4))
    sns.countplot(data=coll_clusters, x="cluster", hue="severity")
    plt.title("Cluster Composition by Severity")
    plt.xlabel("Cluster")
    plt.ylabel("Count")
    plt.legend(title="Severity")
    plt.show()
```

Although the dataset does not contain specific accident severity classifications, the existing numerical variables have revealed differences that are of practical significance.

## 6 Conclusion

This unsupervised learning task shows that:

- Collision records exhibit natural groupings based on numerical attributes.
- K-Means and hierarchical clustering consistently identify 2–3 stable clusters.
- PCA highlights strong visual separation between clusters.
- Hierarchical clustering supports the presence of multiple groups and offers a complementary view of cluster structure.
- Relating clusters to severity or other categorical attributes provides interpretable insight into risk patterns.
- The workflow is adaptable and can incorporate additional variables when available.

Unsupervised learning provides a valuable exploratory tool for understanding traffic accident risks. Future research could incorporate more geographical or demographic variables to further improve the accuracy of the analysis.