---
title: "Regression Coursework Report"
author: "Yuanhao Zhang"
format:
  html:
    toc: true
    code-fold: true
execute:
  working-directory: ../../
---

## 1 Introduction

This report analyses the dataset la_energy.csv, which provides information on housing energy shortfall and related household characteristics.
The goal is to build regression models that predict a continuous energy-related variable.
Based on automatic feature selection, the variable n (representing household sample size per category) was selected as the regression target. This analysis explores the dataset’s correlations, evaluates predictive accuracy, and highlights which factors most influence energy outcomes.

```{python}
import os
from pathlib import Path
import pandas as pd

print("CWD =", os.getcwd())

proj_root = Path(os.getenv("QUARTO_PROJECT_DIR", ".")).resolve()
print("QUARTO_PROJECT_DIR =", os.getenv("QUARTO_PROJECT_DIR"))
print("Resolved project root =", proj_root)

data_path = Path("../../data_cache/la_energy.csv")
print("Reading from:", data_path.resolve())

la_energy = pd.read_csv(data_path)
la_energy.head()
```

## 2 Exploratory Data Analysis (EDA)

We inspect summary statistics for the main numerical variables.

```{python}
la_energy.describe()
```

### Correlation Structure

```{python}
import seaborn as sns
import matplotlib.pyplot as plt

numeric_cols = la_energy.select_dtypes(include="number").columns
corr_mat = la_energy[numeric_cols].corr()

plt.figure(figsize=(6, 5))
sns.heatmap(corr_mat, cmap="coolwarm", annot=False)
plt.title("Correlation Matrix of LA Energy Variables")
plt.show()
```

The related heatmap generated shows:

- shortfall and n have a weak correlation.
- n_rooms and n show some positive association: more rooms often correspond to larger household counts.
- No extremely strong correlations are observed, but some multicollinearity may exist among numeric variables.

## 3 Baseline Model: Linear Regression

We treat a chosen continuous variable (for example, consumption) as the target and
use other numerical variables as features.

```{python}
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

numeric_cols = la_energy.select_dtypes(include="number").columns.tolist()

candidate_targets = [
    c for c in numeric_cols
    if ("energy" in c.lower()) or ("kwh" in c.lower())
]

if candidate_targets:
    target_col = candidate_targets[0]
else:
    target_col = numeric_cols[-1]

print("Chosen target column:", target_col)

feature_cols = [c for c in numeric_cols if c != target_col]

X = la_energy[feature_cols].dropna()
y = la_energy.loc[X.index, target_col]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
y_pred_lin = lin_reg.predict(X_test)

mse_lin = mean_squared_error(y_test, y_pred_lin)
r2_lin = r2_score(y_test, y_pred_lin)

mse_lin, r2_lin
```

The negative R² indicates that the linear model performs slightly worse than simply predicting the mean of n, which suggests no strong linear relationship exists between the selected features and the target variable.

### Predicted vs Actual Plot

```{python}
plt.figure(figsize=(6,5))
sns.scatterplot(x=y_test, y=y_pred_lin)
plt.xlabel(f"Actual {target_col}")
plt.ylabel(f"Predicted {target_col}")
plt.title("Linear Regression Predictions")
plt.show()
```

The scatterplot shows a wide vertical spread around the horizontal axis, confirming that linear regression struggles to capture the relationship. Predictions do not track the actual trend closely.

## 4 Alternative Model: Random Forest Regression

We now fit a Random Forest regressor on the same features.

```{python}
from sklearn.ensemble import RandomForestRegressor

rf_reg = RandomForestRegressor(
n_estimators=200,
random_state=42
)
rf_reg.fit(X_train, y_train)
y_pred_rf = rf_reg.predict(X_test)

mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

mse_rf, r2_rf
```

Random forest usually performs better on non-linear tasks, but in this dataset its performance is also poor.

### Feature Importance

```{python}
import numpy as np

importances = rf_reg.feature_importances_
order = np.argsort(importances)[::-1]

plt.figure(figsize=(6, 4))
sns.barplot(x=importances[order],
y=[numeric_cols[i] for i in order])
plt.title("Random Forest Feature Importance")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()
```

This means that variations in energy shortfall levels across housing categories carry more explanatory power for predicting the target variable (n) than housing size.
Although overall model performance remains poor, this ranking indicates that energy-related scarcity is more closely associated with differences in household counts than the physical characteristics of the dwellings.

This outcome suggests that households experiencing higher energy shortfall levels tend to cluster in categories with different population sizes, whereas room count provides only secondary structural information. Future models could benefit from incorporating additional socioeconomic or geographical indicators to better contextualise the shortfall variable.

## 5 Conclusion

This regression task indicates that:

- Available features are weak predictors of the target variable n.
- Both linear regression and random forest achieve poor R² values, implying limited explanatory power.
- Feature importance reveals shortfall as the most influential predictor.

Future improvements may include:

 - Adding socioeconomic variables,
 - Geographic indicators,
 - Household-level energy consumption,
 - Cross-validation and parameter tuning.